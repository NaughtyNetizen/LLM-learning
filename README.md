# å¤§æ¨¡å‹ä»é›¶å®ç°å­¦ä¹ é¡¹ç›® (LLM Learning)

> ä¸€ä¸ªå®Œæ•´çš„ä»é›¶å¼€å§‹å­¦ä¹ å’Œå®ç°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•™å­¦é¡¹ç›®ã€‚æ”¯æŒä¸­æ–‡è®­ç»ƒã€å¤šç§æ•°æ®é›†è‡ªåŠ¨ä¸‹è½½åŠ GPT-4 Tokenizerã€‚

## ğŸ“š é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æä¾›äº†ä»åº•å±‚åŸç†åˆ°å®Œæ•´å®ç°çš„å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ èµ„æºã€‚ç»è¿‡é‡æ„ï¼Œç°åœ¨æ‹¥æœ‰æ›´æ¸…æ™°çš„é¡¹ç›®ç»“æ„ï¼Œå¹¶åŸç”Ÿæ”¯æŒä¸­æ–‡æ¨¡å‹è®­ç»ƒã€‚

ä¸»è¦åŒ…å«ï¼š
- âœ… **TransformeråŸºç¡€ç»„ä»¶**ï¼šå®Œæ•´å®ç° Attention å’Œ Transformer Block
- âœ… **GPTæ¨¡å‹æ¶æ„**ï¼šæ ‡å‡†çš„ Decoder-only æ¶æ„
- âœ… **å¤šè¯­è¨€æ”¯æŒ**ï¼šæ”¯æŒè‹±æ–‡ (WikiText) å’Œä¸­æ–‡ (Wikipedia) è®­ç»ƒ
- âœ… **é«˜çº§ Tokenizer**ï¼šæ”¯æŒ GPT-2 å’Œ GPT-4 (cl100k_base) Tokenizer
- âœ… **å®Œæ•´è®­ç»ƒæµç¨‹**ï¼šæ”¯æŒæ¢¯åº¦ç´¯ç§¯ã€æ··åˆç²¾åº¦è®­ç»ƒã€æ–­ç‚¹ç»­è®­
- âœ… **è‡ªåŠ¨åŒ–æ•°æ®ç®¡çº¿**ï¼šä¸€é”®ä¸‹è½½å’Œé¢„å¤„ç†é«˜è´¨é‡æ•°æ®é›†

**ä»£ç ç‰¹ç‚¹ï¼š** æ¯è¡Œä»£ç éƒ½æœ‰è¯¦ç»†æ³¨é‡Šï¼ŒåŒ…å«æ•°å­¦å…¬å¼ï¼Œæ˜“äºç†è§£å’Œå­¦ä¹ ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. å‡†å¤‡æ•°æ®

ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬ä¸‹è½½é«˜è´¨é‡æ•°æ®é›†ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰ï¼š

```bash
# ä¸‹è½½æ‰€æœ‰æ•°æ® (WikiText + ä¸­æ–‡ç»´åŸº)
python scripts/download_data.py --dataset all

# ä»…ä¸‹è½½ä¸­æ–‡æ•°æ®
python scripts/download_data.py --dataset chinese
```

### 3. è®­ç»ƒæ¨¡å‹

ä½¿ç”¨ `scripts/train.py` è¿›è¡Œè®­ç»ƒã€‚æ”¯æŒå¤šç§é…ç½®å’Œæ•°æ®é›†ã€‚

```bash
# è®­ç»ƒä¸­æ–‡æ¨¡å‹ (ä½¿ç”¨ GPT-4 Tokenizer)
python scripts/train.py --model_type gpt-micro --dataset chinese --tokenizer cl100k_base --epochs 5

# è®­ç»ƒè‹±æ–‡æ¨¡å‹ (ä½¿ç”¨ GPT-2 Tokenizer)
python scripts/train.py --model_type gpt-mini --dataset shakespeare --tokenizer gpt2
```

**å‚æ•°è¯´æ˜ï¼š**
- `--model_type`: æ¨¡å‹è§„æ¨¡ (`gpt-micro`, `gpt-mini`, `gpt-small`, `gpt2` ç­‰)
- `--dataset`: æ•°æ®é›† (`chinese`, `shakespeare`)
- `--tokenizer`: Tokenizer ç±»å‹ (`gpt2`, `cl100k_base`)

### 4. ç”Ÿæˆæ–‡æœ¬

ä½¿ç”¨ `scripts/generate.py` åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ã€‚

```bash
# ä¸­æ–‡ç”Ÿæˆ
python scripts/generate.py \
    --checkpoint checkpoints/gpt-micro/best_model.pt \
    --prompt "äººå·¥æ™ºèƒ½" \
    --tokenizer cl100k_base \
    --model_type gpt-micro

# è‹±æ–‡ç”Ÿæˆ
python scripts/generate.py \
    --checkpoint checkpoints/gpt-mini/best_model.pt \
    --prompt "To be or not to be" \
    --tokenizer gpt2
```

## ğŸ“‚ é¡¹ç›®ç»“æ„

```
LLM/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ llm_learning/
â”‚       â”œâ”€â”€ model/          # GPTæ¨¡å‹å®šä¹‰ (config.py, model.py)
â”‚       â”œâ”€â”€ modules/        # TransformeråŸºç¡€ç»„ä»¶
â”‚       â”œâ”€â”€ data/           # æ•°æ®å¤„ç† (dataset.py, chinese_dataset.py)
â”‚       â”œâ”€â”€ tokenizer/      # Tokenizerå°è£… (bpe_tokenizer.py)
â”‚       â””â”€â”€ training/       # è®­ç»ƒå™¨å®ç° (trainer.py)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_data.py    # æ•°æ®ä¸‹è½½è„šæœ¬
â”‚   â”œâ”€â”€ train.py            # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ generate.py         # ç”Ÿæˆè„šæœ¬
â”œâ”€â”€ data/                   # æ•°æ®å­˜æ”¾ç›®å½•
â”‚   â””â”€â”€ raw/                # åŸå§‹æ•°æ® (wikitext, chinese_wiki)
â”œâ”€â”€ checkpoints/            # æ¨¡å‹ä¿å­˜ç›®å½•
â””â”€â”€ requirements.txt
```

## ğŸ“ å­¦ä¹ è·¯å¾„

### ç¬¬ä¸€é˜¶æ®µï¼šç†è§£ç»„ä»¶
é˜…è¯» `src/llm_learning/modules/` ä¸‹çš„ä»£ç ï¼Œç†è§£ Self-Attention å’Œ Transformer Block çš„å®ç°ç»†èŠ‚ã€‚

### ç¬¬äºŒé˜¶æ®µï¼šç†è§£æ¨¡å‹ä¸é…ç½®
é˜…è¯» `src/llm_learning/model/`ï¼Œç†è§£ GPT çš„æ•´ä½“æ¶æ„ä»¥åŠ `config.py` ä¸­çš„å‚æ•°è®¾è®¡ã€‚

### ç¬¬ä¸‰é˜¶æ®µï¼šæ•°æ®ä¸Tokenizer
é˜…è¯» `src/llm_learning/data/` å’Œ `src/llm_learning/tokenizer/`ï¼Œäº†è§£å¦‚ä½•å¤„ç†å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®ä»¥åŠ BPE ç¼–ç åŸç†ã€‚

### ç¬¬å››é˜¶æ®µï¼šè®­ç»ƒä¸ä¼˜åŒ–
é˜…è¯» `src/llm_learning/training/`ï¼ŒæŒæ¡è®­ç»ƒå¾ªç¯ã€æ¢¯åº¦ç´¯ç§¯ã€æ··åˆç²¾åº¦è®­ç»ƒç­‰å·¥ç¨‹æŠ€å·§ã€‚

## â“ å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆä¸­æ–‡è¾“å‡ºæ˜¯ä¹±ç ï¼Ÿ**
A: è¯·ç¡®ä¿ä½¿ç”¨ `--tokenizer cl100k_base` è¿›è¡Œè®­ç»ƒå’Œç”Ÿæˆï¼ŒGPT-2 çš„é»˜è®¤ tokenizer å¯¹ä¸­æ–‡æ”¯æŒè¾ƒå·®ã€‚åŒæ—¶ç¡®ä¿è®­ç»ƒæ•°æ®æ˜¯ä¸­æ–‡æ•°æ® (`--dataset chinese`)ã€‚

**Q: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ**
A: 
1. å‡å° `--batch_size`
2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ (`gpt-micro`)
3. å¢åŠ  `--gradient_accumulation_steps` (åœ¨ä»£ç ä¸­è°ƒæ•´)

**Q: å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®ï¼Ÿ**
A: å°†ä½ çš„æ–‡æœ¬æ–‡ä»¶å‘½åä¸º `train.txt` å’Œ `validation.txt`ï¼Œæ”¾å…¥ `data/raw/your_dataset/` ç›®å½•ï¼Œå¹¶ä¿®æ”¹ `src/llm_learning/data/dataset.py` ä¸­çš„åŠ è½½é€»è¾‘ã€‚

## ğŸ“„ è®¸å¯è¯

MIT License
